<p align="center"> <img src="https://github.com/kingTLE/kingTLE/blob/main/header.png"/> </p>

<h1 align="center">
  Literary-Alpaca2
</h1>
<p align="center">
  <font face="é»‘ä½“" color=orange size="6"> ä»è¯è¡¨åˆ°å¾®è°ƒè¿™å°±æ˜¯ä½ éœ€è¦çš„ä¸€åˆ‡ </font>
</p>

</br></br>


## ğŸ—‚ï¸ ä½¿ç”¨æŒ‡å—
- [ğŸ”¥ é¡¹ç›®ä»‹ç»](#-é¡¹ç›®ä»‹ç»)
- [ğŸ“ è®­ç»ƒæ•°æ®](#-è®­ç»ƒæ•°æ®)
- [â¬ æ¨¡å‹éƒ¨ç½²](#-æ¨¡å‹éƒ¨ç½²)
  - [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
    - [åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹](#åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹)
    - [åŸºäºLiteraryAlpaca2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹Chat](#åŸºäºLiteraryAlpaca2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹Chat)
  - [æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹](#æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹)
  - [Gradioå¿«é€Ÿæ­å»ºé—®ç­”å¹³å°](#gradioå¿«é€Ÿæ­å»ºé—®ç­”å¹³å°)
- [è¯è¡¨è®­ç»ƒ](#è¯è¡¨è®­ç»ƒ)
- [é¢„è®­ç»ƒ](#é¢„è®­ç»ƒ)
- [å¾®è°ƒ](#å¾®è°ƒ)
  - [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
  - [å¾®è°ƒè„šæœ¬](#å¾®è°ƒè„šæœ¬)
- [å‚è€ƒè®ºæ–‡](#å‚è€ƒè®ºæ–‡)



## ğŸ”¥ é¡¹ç›®ä»‹ç»


æœ¬ä»“åº“å°†å±•ç¤ºå¦‚ä½•ä»è¯è¡¨å¼€å§‹æ„å»ºè‡ªå·±çš„è¯è¡¨ä¸ä½¿ç”¨åŸºåº§æ¨¡å‹é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹
ä»“åº“ä¸­çš„ä»£ç ç¤ºä¾‹ä¸»è¦æ˜¯åŸºäºLlama2çš„Hugging Faceç‰ˆæœ¬è¿›è¡Œè®­ç»ƒã€‚


## ğŸ“ è®­ç»ƒæ•°æ®
| ç±»å‹                                                       | æè¿°                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------ |
| ç½‘ç»œå°è¯´                                                   | é«˜è´¨é‡é•¿æ–‡æœ¬æ•°æ® |
| [Math23K](https://opendatalab.org.cn/Math23K)               | ä¸­æ–‡æ•°å­¦é—®é¢˜                                          |
| [LCCC](https://github.com/thu-coai/CDial-GPT)               | ä¸­æ–‡å¼€æºå¯¹è¯é›†                                       |

</br></br>
è¯è¡¨ä¸é¢„è®­ç»ƒé˜¶æ®µæ•°æ®å¯¹æ¯”å›¾ï¼š
<p align="center"> <img src="img/data_comparison.png" width=80%/> </p>

## â¬ æ¨¡å‹éƒ¨ç½²

<p>Metaå®˜æ–¹çš„ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/meta-llama</p>

ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ã€LoRAå‚æ•°ã€chatæ¨¡å‹éƒ½å·²ä¸Šä¼ è‡³[Hugging Face](https://huggingface.co/taotie1) ç›®å‰åªæœ‰13Bæ¨¡å‹ã€‚

### æ¨¡å‹ä¸‹è½½

#### åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹

|  ç±»åˆ«        | ğŸ¤—æ¨¡å‹åç§°   | åŸºåº§æ¨¡å‹          |   ä¸‹è½½åœ°å€          |
|  ----------  | ---------- |  ----------------- | ------------------- |
|  é¢„è®­ç»ƒ | taotie1/literary-alpaca2-13B |     meta-llama/Llama-2-13b-hf     |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/taotie1/literary-alpaca2-13B) |
|  LoRA | taotie1/literary-alpaca2-13B-lora |      taotie1/literary-alpaca2-13B     |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/taotie1/literary-alpaca2-13B-lora) |
#### åŸºäºLiteraryAlpaca2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹Chat
|  ç±»åˆ«           | ğŸ¤—æ¨¡å‹åç§°        | ä¸‹è½½åœ°å€                                                 |
| --------------- | ---------------    |  ------------------------------------------------------------ |
|  Chat  |  taotie1/literary-alpaca2-13B-chat  | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/taotie1/literary-alpaca2-13B-chat) |


### æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹
æ ¹æ®[requirements.txt](https://github.com/kingTLE/literary-alpaca2/blob/main/requirements.txt)å®‰è£…ç¯å¢ƒä¾èµ–
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained('taotie1/literary-alpaca2-13B-chat',device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)
model =model.eval()
tokenizer = AutoTokenizer.from_pretrained('taotie1/literary-alpaca2-13B-chat',use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
input_ids = tokenizer(['<s>Human: ä»‹ç»ä¸€ä¸‹ä¸­å›½\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```

### Gradioå¿«é€Ÿæ­å»ºé—®ç­”å¹³å°

åŸºäºgradioæ­å»ºçš„é—®ç­”ç•Œé¢ï¼Œå®ç°äº†æµå¼çš„è¾“å‡ºï¼Œå°†ä¸‹é¢ä»£ç å¤åˆ¶åˆ°æ§åˆ¶å°è¿è¡Œï¼Œä»¥ä¸‹ä»£ç ä»¥Atom-7Bæ¨¡å‹ä¸ºä¾‹ï¼Œ<font color="#006600">ä¸åŒæ¨¡å‹åªéœ€ä¿®æ”¹ä¸€ä¸‹ä»£ç é‡Œçš„æ¨¡å‹åç§°å°±å¥½äº†ğŸ˜Š</font><br/>
```
python examples/chat_gradio.py --model_name_or_path FlagAlpha/Atom-7B
```
## è¯è¡¨è®­ç»ƒ

å…ˆå¯¹ä½ çš„è®­ç»ƒæ•°æ®è¿›è¡Œ[å‘½åæ¸…æ´—](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/Batch_Rename.py)ã€å¯é€‰ã€‘</br></br>
é€‰æ‹©è¿è¡Œ[éšæœºæ¸…æ´—ä»£ç ](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/random_sample.py)æˆ–[å…¨éƒ¨æ¸…æ´—](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/clear.py)ï¼Œåœ¨[ill_ocr_regex.txt](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/ill_ocr_regex.txt)ä¸­å¯ä»¥è‡ªå®šä¹‰ä½ çš„æ­£åˆ™ã€‚

è¿è¡Œ[full_sample_extraction.py](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/full_sample_extraction.py)æŠŠæ•°æ®åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ã€‚

å‚ç…§[train-chinese-tokenizer.ipynb](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/train-chinese-tokenizer.ipynb)è¿›è¡Œè¯è¡¨è®­ç»ƒï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚ä¿®æ”¹ä»£ç ã€‚
è®­ç»ƒå®ŒæˆåæŠŠä½ çš„è¯è¡¨æ”¾å…¥my-tokenizerç›®å½•ä¸‹ã€‚æŒ‰ç…§ä¸‹é¢æ–¹å¼å’ŒåŸllama2çš„tokenizeråˆå¹¶
```
bashè¿è¡Œ
'
Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
python incorporation.py
'
```
è¿è¡Œ[text.py](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/text.py)è¿›è¡Œæµ‹è¯•è¯è¡¨æ•ˆæœ

## é¢„è®­ç»ƒ
æœ¬ä»“åº“è®­ç»ƒä»£ç ä½¿ç”¨[DeepSpeed](https://github.com/microsoft/DeepSpeed)åŠ é€Ÿ
- æ¨¡å‹é¢„è®­ç»ƒè„šæœ¬ï¼š[train/GPU/pretrain-peft1.sh](https://github.com/kingTLE/literary-alpaca2/tree/main/train/GPU/pretrain-peft1.sh)
- é¢„è®­ç»ƒå®ç°ä»£ç ï¼š[train/GPU/pretrain-peft1.py](https://github.com/kingTLE/literary-alpaca2/tree/main/train/GPU/pretrain-peft1.py)


## å¾®è°ƒ

### æ•°æ®å‡†å¤‡
åœ¨dataç›®å½•ä¸‹æä¾›äº†ä¸€ä»½ç”¨äºæ¨¡å‹sftçš„æ•°æ®æ ·ä¾‹ï¼š
- è®­ç»ƒæ•°æ®ï¼š[data/train_sft.csv](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/data/train_sft.csv)
- éªŒè¯æ•°æ®ï¼š[data/dev_sft.csv](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/data/dev_sft.csv)

æ¯ä¸ªcsvæ–‡ä»¶ä¸­åŒ…å«ä¸€åˆ—â€œtextâ€ï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ¯ä¸ªè®­ç»ƒæ ·ä¾‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å°†é—®é¢˜å’Œç­”æ¡ˆç»„ç»‡ä¸ºæ¨¡å‹è¾“å…¥ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è‡ªå®šä¹‰è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼š
```
"<s>Human: "+é—®é¢˜+"\n</s><s>Assistant: "+ç­”æ¡ˆ
```
ä¾‹å¦‚ï¼Œ
```
<s>Human: ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚</s><s>Assistant: å› ä¸ºåœ°çƒæ˜¯ç›®å‰ä¸ºæ­¢å”¯ä¸€å·²çŸ¥å­˜åœ¨ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚</s>
```
### å¾®è°ƒè„šæœ¬

LoRAå¾®è°ƒè„šæœ¬è§ï¼š[train/sft/finetune_lora.sh](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/train/sft/finetune_lora.sh)ï¼Œå…³äºLoRAå¾®è°ƒçš„å…·ä½“å®ç°ä»£ç è§[train/sft/finetune_clm_lora.py](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/train/sft/finetune_clm_lora.py)ï¼Œå•æœºå¤šå¡çš„å¾®è°ƒå¯ä»¥é€šè¿‡ä¿®æ”¹è„šæœ¬ä¸­çš„`--include localhost:0`æ¥å®ç°ã€‚




## å‚è€ƒè®ºæ–‡

* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
* [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
* [Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca](https://arxiv.org/abs/2304.08177)

<p align="center" width="100%">
<img src="https://starchart.cc/kingTLE/literary-alpaca2.svg" alt="Star History" style="width: 100%; display: block; margin: auto;">
</p>
