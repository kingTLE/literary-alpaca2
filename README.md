<p align="center"> <img src="https://github.com/kingTLE/kingTLE/blob/main/header.png" width=80%/> </p>

<h1 align="center">
  Literary-Alpaca2
</h1>
<p align="center">
  <font face="é»‘ä½“" color=orange size="6"> ä»è¯è¡¨åˆ°å¾®è°ƒè¿™å°±æ˜¯ä½ éœ€è¦çš„ä¸€åˆ‡ </font>
</p>

</br></br>


## ğŸ—‚ï¸ ä½¿ç”¨æŒ‡å—
- [ğŸ”¥ é¡¹ç›®ä»‹ç»](#-é¡¹ç›®ä»‹ç»)
- [ğŸ“ è®­ç»ƒæ•°æ®](#-è®­ç»ƒæ•°æ®)
- [â¬ æ¨¡å‹éƒ¨ç½²](#-æ¨¡å‹éƒ¨ç½²)
  - [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
    - [åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹](#åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹)
    - [åŸºäºLiteraryAlpaca2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹Chat](#åŸºäºLiteraryAlpaca2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹Chat)
  - [æ¨¡å‹è°ƒç”¨ç¤ºä¾‹](#æ¨¡å‹è°ƒç”¨ç¤ºä¾‹)
- [è¯è¡¨è®­ç»ƒ](#è¯è¡¨è®­ç»ƒ)
- [é¢„è®­ç»ƒ](#é¢„è®­ç»ƒ)
- [å¾®è°ƒ](#å¾®è°ƒ)
  - [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
  - [å¾®è°ƒè„šæœ¬](#å¾®è°ƒè„šæœ¬)
- [å‚è€ƒè®ºæ–‡](#å‚è€ƒè®ºæ–‡)



## ğŸ”¥ é¡¹ç›®ä»‹ç»


æœ¬ä»“åº“å°†å±•ç¤ºå¦‚ä½•ä»è¯è¡¨å¼€å§‹æ„å»ºè‡ªå·±çš„è¯è¡¨ä¸ä½¿ç”¨åŸºåº§æ¨¡å‹é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹
ä»“åº“ä¸­çš„ä»£ç ç¤ºä¾‹ä¸»è¦æ˜¯åŸºäºLlama2çš„Hugging Faceç‰ˆæœ¬è¿›è¡Œè®­ç»ƒã€‚


## ğŸ“ è®­ç»ƒæ•°æ®
| ç±»å‹                                                       | æè¿°                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------ |
| ç½‘ç»œå°è¯´                                                   | é«˜è´¨é‡é•¿æ–‡æœ¬æ•°æ® |
| [Math23K](https://opendatalab.org.cn/Math23K)               | ä¸­æ–‡æ•°å­¦é—®é¢˜                                          |
| [LCCC](https://github.com/thu-coai/CDial-GPT)               | ä¸­æ–‡å¼€æºå¯¹è¯é›†                                       |

</br></br>
è¯è¡¨ä¸é¢„è®­ç»ƒé˜¶æ®µæ•°æ®å¯¹æ¯”å›¾ï¼š
<p align="center"> <img src="img/data_comparison.png" width=80%/> </p>

## â¬ æ¨¡å‹éƒ¨ç½²

<p>Metaå®˜æ–¹çš„ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/meta-llama</p>

ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ã€LoRAå‚æ•°ã€chatæ¨¡å‹éƒ½å·²ä¸Šä¼ è‡³[Hugging Face](https://huggingface.co/taotie1) ç›®å‰åªæœ‰13Bæ¨¡å‹ã€‚

### æ¨¡å‹ä¸‹è½½

#### åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹

|  ç±»åˆ«        | ğŸ¤—æ¨¡å‹åç§°   | åŸºåº§æ¨¡å‹          |   ä¸‹è½½åœ°å€          |
|  ----------  | ---------- |  ----------------- | ------------------- |
|  é¢„è®­ç»ƒ | taotie1/literary-alpaca2-13B |     meta-llama/Llama-2-13b-hf     |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/taotie1/literary-alpaca2-13B) |
|  LoRA | taotie1/literary-alpaca2-13B-lora |      taotie1/literary-alpaca2-13B     |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/taotie1/literary-alpaca2-13B-lora) |
#### åŸºäºLiteraryAlpaca2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹Chat
|  ç±»åˆ«           | ğŸ¤—æ¨¡å‹åç§°        | ä¸‹è½½åœ°å€                                                 |
| --------------- | ---------------    |  ------------------------------------------------------------ |
|  Chat  |  taotie1/literary-alpaca2-13B-chat  | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/taotie1/literary-alpaca2-13B-chat) |


### æ¨¡å‹è°ƒç”¨ç¤ºä¾‹
æ ¹æ®[requirements.txt](https://github.com/kingTLE/literary-alpaca2/blob/main/requirements.txt)å®‰è£…ç¯å¢ƒä¾èµ–ï¼Œtorchè¯·æ ¹æ®è‡ªå·±çš„è®¾å¤‡é€‰æ‹©ç‰ˆæœ¬å®‰è£…ã€‚
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained('taotie1/literary-alpaca2-13B-chat',device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)
model =model.eval()
tokenizer = AutoTokenizer.from_pretrained('taotie1/literary-alpaca2-13B-chat',use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
input_ids = tokenizer(['<s>Human: ä»€ä¹ˆæ˜¯è®¡ç®—æœº\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```

## è¯è¡¨è®­ç»ƒ

å…ˆå¯¹ä½ çš„è®­ç»ƒæ•°æ®è¿›è¡Œ[å‘½åæ¸…æ´—](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/Batch_Rename.py)ã€å¯é€‰ã€‘</br></br>
é€‰æ‹©è¿è¡Œ[éšæœºæ¸…æ´—ä»£ç ](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/random_sample.py)æˆ–[å…¨éƒ¨æ¸…æ´—](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/clear.py)ï¼Œåœ¨[ill_ocr_regex.txt](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/ill_ocr_regex.txt)ä¸­å¯ä»¥è‡ªå®šä¹‰ä½ çš„æ­£åˆ™ã€‚

è¿è¡Œ[full_sample_extraction.py](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/full_sample_extraction.py)æŠŠæ•°æ®åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ã€‚

å‚ç…§[train-chinese-tokenizer.ipynb](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/train-chinese-tokenizer.ipynb)è¿›è¡Œè¯è¡¨è®­ç»ƒï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚ä¿®æ”¹ä»£ç ã€‚
è®­ç»ƒå®ŒæˆåæŠŠä½ çš„è¯è¡¨æ”¾å…¥my-tokenizerç›®å½•ä¸‹ã€‚æŒ‰ç…§ä¸‹é¢æ–¹å¼å’ŒåŸllama2çš„tokenizeråˆå¹¶
```
bashè¿è¡Œ
'
Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
python incorporation.py
'
```
è¿è¡Œ[text.py](https://github.com/kingTLE/literary-alpaca2/tree/main/chinese-tokenizer/text.py)è¿›è¡Œæµ‹è¯•è¯è¡¨æ•ˆæœ

## é¢„è®­ç»ƒ
æœ¬ä»“åº“è®­ç»ƒä»£ç ä½¿ç”¨[DeepSpeed](https://github.com/microsoft/DeepSpeed)åŠ é€Ÿ
- è¯·åœ¨[é¢„è®­ç»ƒè„šæœ¬ pretrain-peft1.sh](https://github.com/kingTLE/literary-alpaca2/tree/main/train/GPU/pretrain-peft1.sh)ä¸­ä¿®æ”¹ä½ ç›¸åº”çš„output_modelã€datasetã€pretrained_model_nameã€tokenizer_nameçš„è·¯å¾„,å¦‚æœéœ€è¦ä¸Šä¼ åˆ°ä½ çš„Hugging Faceä»“åº“ï¼Œè¯·æŠŠ    --push_to_hub è®¾ç½®ä¸ºtrueå¹¶åœ¨  --hub_tokenå‚æ•°å¡«å…¥ä½ çš„tokenï¼Œå¹¶ä¿®æ”¹    --hub_model_id
- å¤šæœºå¤šå¡è¯·ä¿®æ”¹--nnodes å’Œ --nproc_per_node å‚æ•°
```
å¦‚ä¸¤æœº8å¡ï¼štorchrun --nnodes 2 --nproc_per_node 8
```

- ä½ å¯ä»¥åˆ é™¤[é¢„è®­ç»ƒä»£ç  pretrain-peft1.py](https://github.com/kingTLE/literary-alpaca2/tree/main/train/GPU/pretrain-peft1.py)ä¸­çš„ä¸‹é¢å†…å®¹å®ç°å…¨å‚é¢„è®­ç»ƒ
```
    for name, param in model.named_parameters():
        if "model.embed_tokens" not in name:
            param.requires_grad = False
        else:
            param.requires_grad = True
```
- å¦‚æœéš¾ä»¥æ”¶æ•›æˆ–å†…å­˜ä¸è¶³è¯·ä½¿ç”¨[é¢„è®­ç»ƒè„šæœ¬2 pretrain-peft2.sh](https://github.com/kingTLE/literary-alpaca2/tree/main/train/GPU/pretrain-peft1.sh)ï¼Œè¿™å°†å…è®¸ä½ è®­ç»ƒé‡åŒ–æ¨¡å‹ï¼Œä½ å¯ä»¥é€šè¿‡è°ƒæ•´ä¸‹é¢å‚æ•°é€‚åº”ä½ çš„éœ€æ±‚
```
      --load_in_kbits è®¾ç½®é‡åŒ–,ä¸ä¸º4æˆ–8åˆ™ä¸å¯ç”¨é‡åŒ–
      --bf16 | --fp16 å¯ç”¨bf16éœ€è¦gpuç¡¬ä»¶æ”¯æŒ
å¦‚æœå‡ºç°OOMè¯·åœ¨deepspeed_config_peft2.jsoné…ç½®ä¸­æ·»åŠ ï¼š
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },
```
-ä½¿ç”¨[é¢„è®­ç»ƒè„šæœ¬2 pretrain-peft2.sh](https://github.com/kingTLE/literary-alpaca2/tree/main/train/GPU/pretrain-peft1.sh)ä¼šç”Ÿæˆloraå‚æ•°ï¼Œå¯ä»¥è¿è¡Œä¿®æ”¹è‡ª[Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/tree/main)çš„[merge_lora_low_mem.py](https://github.com/kingTLE/literary-alpaca2/tree/main/train/merge_lora_low_mem.py)è„šæœ¬è¿›è¡Œåˆå¹¶

```
python merge_lora_low_mem.py\
    --base_model /root/LiteraryAlpaca2 \
    --lora_model /root/autodl-tmp/LiteraryAlpaca2-lora \
    --output_type huggingface \
    --output_dir /root/autodl-tmp/LiteraryAlpaca2

```

## å¾®è°ƒ

### æ•°æ®å‡†å¤‡
ä½¿ç”¨sftç›®å½•ä¸‹çš„è½¬æ¢è„šæœ¬å¯ä»¥å°†æ•°æ®é›†è½¬æ¢ä¸ºéœ€è¦çš„è®­ç»ƒæ ¼å¼ï¼š
- Stanford-alpacagæ ¼å¼ï¼š[LCCDè½¬æ¢è„šæœ¬](https://github.com/kingTLE/literary-alpaca2/tree/main/sft/Stanford_lccd.py) [Math23Kè½¬æ¢è„šæœ¬](https://github.com/kingTLE/literary-alpaca2/tree/main/sft/Stanford_math.py)

æ¯ä¸ªcsvæ–‡ä»¶ä¸­åŒ…å«ä¸€åˆ—â€œtextâ€ï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ¯ä¸ªè®­ç»ƒæ ·ä¾‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å°†é—®é¢˜å’Œç­”æ¡ˆç»„ç»‡ä¸ºæ¨¡å‹è¾“å…¥ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è‡ªå®šä¹‰è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼š

- csvæ ¼å¼ï¼š[LCCDè½¬æ¢è„šæœ¬](https://github.com/kingTLE/literary-alpaca2/tree/main/sft/lccd_csv.py) [Math23Kè½¬æ¢è„šæœ¬](https://github.com/kingTLE/literary-alpaca2/tree/main/sft/math_csv.py)

è½¬æ¢çš„csvæ–‡ä»¶ä¸­åŒ…å«ä¸€åˆ—â€œtextâ€ï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ¯ä¸ªè®­ç»ƒæ ·ä¾‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å°†é—®é¢˜å’Œç­”æ¡ˆç»„ç»‡ä¸ºæ¨¡å‹è¾“å…¥ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è‡ªå®šä¹‰è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼š
```
"<s>Human: "+é—®é¢˜+"\n</s><s>Assistant: "+ç­”æ¡ˆ
```
ä¾‹å¦‚ï¼Œ
```
<s>Human: ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚</s><s>Assistant: å› ä¸ºåœ°çƒæ˜¯ç›®å‰ä¸ºæ­¢å”¯ä¸€å·²çŸ¥å­˜åœ¨ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚</s>
```
### å¾®è°ƒè„šæœ¬

LoRAå¾®è°ƒè„šæœ¬è§ï¼š[train/sft/finetune_lora.sh](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/train/sft/finetune_lora.sh)ï¼Œå…³äºLoRAå¾®è°ƒçš„å…·ä½“å®ç°ä»£ç è§[train/sft/finetune_clm_lora.py](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/train/sft/finetune_clm_lora.py)




## å‚è€ƒè®ºæ–‡

* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
* [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
* [Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca](https://arxiv.org/abs/2304.08177)

<p align="center" width="100%">
<img src="https://starchart.cc/kingTLE/literary-alpaca2.svg" alt="Star History" style="width: 100%; display: block; margin: auto;">
</p>
